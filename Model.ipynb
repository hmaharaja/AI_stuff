{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmaharaja/AI_stuff/blob/Steel-defect-classification/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APvBliyiiOVP",
        "colab_type": "text"
      },
      "source": [
        "# **1. Importing Kaggle Data**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvGgfZFc9oqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Link to competition: https://www.kaggle.com/c/severstal-steel-defect-detection/overview\n",
        "#Upload the Kaggle JSON file (download it from the drive to your computer first)\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEoyul_ovMji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mount the kaggle.json file\n",
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu52plUlxddU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract required files\n",
        "!kaggle competitions download -c severstal-steel-defect-detection -q\n",
        "!unzip -q train.csv.zip\n",
        "!unzip -q train_images.zip -d train_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r2VpxnftjQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Include all import statements required over here\n",
        "import csv\n",
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from PIL import Image, ImageDraw\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import cv2\n",
        "from scipy import ndimage, misc\n",
        "import itertools\n",
        "from skimage import io, transform\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, utils, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMQKJpvFc4zS",
        "colab_type": "text"
      },
      "source": [
        "**List of global variables used in the file and their purpose (update as needed)**\n",
        "\n",
        "`trainingDataPath` : path to the train.csv file\n",
        "\n",
        "`data` : array extracted from train.csv file\n",
        "\n",
        "`filenameAndClassIndex` : dictionary with all `ImageId_ClassId` strings from train.csv file as keys (e.g. '002cc93b.jpg_1') and row position in `data` as values (e.g. 1).  Use it to index `data` by filename (e.g. to get encodedPixels for a filename and known class) \n",
        "\n",
        "`numInClassCounter` : An array of counters for the number of images with defects in each class. `numInClassCounter[0]` is the number of images w/ no defects, `numInClassCounter[1]` is the number of images w/ class 1 defects... \n",
        "\n",
        "`filenamesByClass` : a 2D list where the first dimension corresponds to the classID (0: No defects, ..., 4: Class 4) and the second dimension is a list of filenames with **any** defects in that class\n",
        "\n",
        "`uniqueFilenamesByClass` : a 2D list where the first dimension corresponds to the classID (0: No defects, ..., 4: Class 4) and the second dimension is a list of filenames that **only** have defects in that one class\n",
        "\n",
        "`numClasses = 5`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgkxl0dJiaKN",
        "colab_type": "text"
      },
      "source": [
        "# **2. Import the dataset**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgT7-0po5Az5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the dataset and list of filenames\n",
        "\n",
        "def removeDuplicates(lst):\n",
        "    #Remove duplicate values from a list\n",
        "    return list(dict.fromkeys(lst))\n",
        "\n",
        "#Returns the run-length encoded pixels for a filename and given classID from 1 to 4\n",
        "def getEncodedPixels(filename, classID):\n",
        "    return data[filenameAndClassIndex[filename + \"_\" + str(classID)]][1]\n",
        "\n",
        "#Load train.csv as a list\n",
        "trainingDataPath = \"/content/train.csv\"\n",
        "with open(trainingDataPath, 'r') as file:\n",
        "    data = list(csv.reader(file, delimiter=\",\"))\n",
        "\n",
        "filenameAndClassIndex = {} #dictionary to index data array based on first column's value (ImageId_ClassId)   \n",
        "filenames = []\n",
        "\n",
        "for i in range(1, len(data)):\n",
        "    filenameAndClassIndex[data[i][0]] = i\n",
        "    filenames.append(data[i][0].split(\"_\")[0])\n",
        "filenames = removeDuplicates(filenames) # remove duplicates from the list of filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGy0F7Fui1ie",
        "colab_type": "text"
      },
      "source": [
        "# **3. Data Exploration**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_hCKDAd07mt",
        "colab_type": "text"
      },
      "source": [
        "**3.1 - Descriptive Statistics**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_xRLaYBaje1",
        "colab_type": "text"
      },
      "source": [
        "3.2.1 - Example images without any processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYbY5ZxdaJZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Look at a few images to try and identify the defects\n",
        "#Plot a few images from each class without any masks\n",
        "pltFilenames = []\n",
        "\n",
        "numImagesPerClass = 3\n",
        "\n",
        "fig, axs = plt.subplots(numClasses, numImagesPerClass, figsize=(numClasses*4,numImagesPerClass*4))\n",
        "\n",
        "for i in range(numClasses):\n",
        "    for j in range(numImagesPerClass): \n",
        "        #choose a random image with only one defect for each class \n",
        "        random.seed(i+j+21)\n",
        "        pltFilenames.append(uniqueFilenamesByClass[i][random.randint(0, len(uniqueFilenamesByClass[i]))])\n",
        "        \n",
        "        filename = pltFilenames[-1]\n",
        "\n",
        "        #Plot the image as is\n",
        "        img = RGBToGrey(imageToRGBArray(getImage(filename)))\n",
        "        axs[i, j].imshow(img, cmap = \"gray\")\n",
        "        axs[i, j].set_title(\"Class \" + str(i) + \" | Picture \" + str(j+1) + \" | Filename: \" + str(filename))\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2sp-M4xaqsb",
        "colab_type": "text"
      },
      "source": [
        "3.2.2 - Example images with ground-truth pixel defect masks applied in red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_3FRRGVnLRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the same images from above, but with masks to see what the defects are\n",
        "fig, axs = plt.subplots(numClasses, numImagesPerClass, figsize=(numClasses*4,numImagesPerClass*4))\n",
        "filenameIter = iter(pltFilenames)\n",
        "for i in range(numClasses):\n",
        "    for j in range(numImagesPerClass): \n",
        "        #Plot the image with a defect mask applied\n",
        "        filename = next(filenameIter)\n",
        "        if i != 0:\n",
        "            if getEncodedPixels(filename, i) != \"\": #apply the mask only if there is a value of encodedPixels\n",
        "                img = imageToRGBArray(applyMask(getImage(filename), getEncodedPixels(filename, i), (255, 0, 0, 30)))\n",
        "        else:\n",
        "            #If no mask (i.e. class 0), just plot the image\n",
        "            img = imageToRGBArray(getImage(filename))\n",
        "        \n",
        "        axs[i, j].imshow(img)\n",
        "        axs[i, j].set_title(\"Class \" + str(i) + \" | Picture \" + str(j+1) + \" | Filename: \" + str(filename))\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdGMGLSSa57s",
        "colab_type": "text"
      },
      "source": [
        "3.2.3 - Example images processed with histogram equalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at6lDSO6SdQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the same images from above without masks, but this time with histogram equalization\n",
        "fig, axs = plt.subplots(numClasses, numImagesPerClass, figsize=(numClasses*4,numImagesPerClass*4))\n",
        "filenameIter = iter(pltFilenames)\n",
        "for i in range(numClasses):\n",
        "    for j in range(numImagesPerClass): \n",
        "        #Plot the image with a defect mask applied\n",
        "        filename = next(filenameIter)\n",
        "        img = imgHistogramEqualization(RGBToGrey(imageToRGBArray(getImage(filename))))[0]\n",
        "        axs[i, j].imshow(img, cmap = \"gray\")\n",
        "        axs[i, j].set_title(\"Class \" + str(i) + \" | Picture \" + str(j+1) + \" | Filename: \" + str(filename))\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDZ8dRyVbD0F",
        "colab_type": "text"
      },
      "source": [
        "3.2.4 - Example images processed with thresholding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3VWnrpuJS46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the same images from above without masks, but this time with thresholding\n",
        "fig, axs = plt.subplots(numClasses, numImagesPerClass, figsize=(numClasses*4,numImagesPerClass*4))\n",
        "filenameIter = iter(pltFilenames)\n",
        "for i in range(numClasses):\n",
        "    for j in range(numImagesPerClass): \n",
        "        #Plot the image with a defect mask applied\n",
        "        filename = next(filenameIter)\n",
        "        img = thresholdImg(imageToRGBArray(getImage(filename)), 0.3)\n",
        "        axs[i, j].imshow(img)\n",
        "        axs[i, j].set_title(\"Class \" + str(i) + \" | Picture \" + str(j+1) + \" | Filename: \" + str(filename))\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjGh8kjqbMuU",
        "colab_type": "text"
      },
      "source": [
        "3.2.5 - Example images processed with Sobel filters (horizontal/vertical edge detection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azbBocBwbuNs",
        "colab_type": "text"
      },
      "source": [
        "refer to http://www.cs.cmu.edu/~16385/s17/Slides/4.0_Image_Gradients_and_Gradient_Filtering.pdf for more information on this filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRN_PzcPVZmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the same images from above without masks, but this time with a Sobel filter\n",
        "fig, axs = plt.subplots(numClasses, numImagesPerClass, figsize=(numClasses*4,numImagesPerClass*4))\n",
        "filenameIter = iter(pltFilenames)\n",
        "for i in range(numClasses):\n",
        "    for j in range(numImagesPerClass): \n",
        "        filename = next(filenameIter)\n",
        "        img = sobelImg(RGBToGrey(imageToRGBArray(getImage(filename))))\n",
        "        axs[i, j].imshow(img, cmap = \"gray\")\n",
        "        axs[i, j].set_title(\"Class \" + str(i) + \" | Picture \" + str(j+1) + \" | Filename: \" + str(filename))\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzxdowAKbYsK",
        "colab_type": "text"
      },
      "source": [
        "3.2.6 - Example images processed with a Laplacian derivative filter and gaussian smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN-whdZHblgm",
        "colab_type": "text"
      },
      "source": [
        "refer to http://www.cs.cmu.edu/~16385/s17/Slides/4.0_Image_Gradients_and_Gradient_Filtering.pdf for more information on this filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXWU3vFaSgOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the same images from above without masks, but this time with a Laplace filter with gaussian smoothing\n",
        "fig, axs = plt.subplots(numClasses, numImagesPerClass, figsize=(numClasses*4,numImagesPerClass*4))\n",
        "filenameIter = iter(pltFilenames)\n",
        "for i in range(numClasses):\n",
        "    for j in range(numImagesPerClass): \n",
        "        filename = next(filenameIter)\n",
        "        img = gaussianLaplaceImg(RGBToGrey(imageToRGBArray(getImage(filename))))\n",
        "        axs[i, j].imshow(img, cmap = \"gray\")\n",
        "        axs[i, j].set_title(\"Class \" + str(i) + \" | Picture \" + str(j+1) + \" | Filename: \" + str(filename))\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAQ2NT8uHoD9",
        "colab_type": "text"
      },
      "source": [
        "**Conclusions:**\n",
        "\n",
        "*From pie chart*\n",
        "- Of the images with defects, class 3 is the most common, followed by classes 1, 4, and 2\n",
        "- A majority of the images have no defects\n",
        "\n",
        "*From comparison matrix*\n",
        "- The most common combination of multi-class defects in an image is a combination of class 3 and class 4. This is followed by class 1 and class 3, class 1 and class 2, class 2 and class 3, and class 2 and class 4.\n",
        "\n",
        "*From pixel intensity histogram*\n",
        "- Class 2 defects are generally distributed over lower intensities than any other class\n",
        "- Classes 1 and 3 have similar distributions, meaning it may potentially be difficult to distinguish between them\n",
        "- The majority number of pixels for images without defects (i.e. class 0's peak on the histogram), is at a higher intensity than the peak for any other class\n",
        "- Class 4 pictures do not seem to peak at a certain intensity; the pixels have a relatively equal distribution of intensities \n",
        "\n",
        "*From visualizing defect images*\n",
        "- Class 1 defects seem to be tiny transverse cracks/indentations or potentially pitting due to their relatively small size compared to the other defect classes(i.e. crack length extends into the page, but is being viewed from above) \n",
        "- Class 2 defects seem the most difficult to detect, they are barely noticeable vertical scratches on the surface. This class may be difficult to detect due to this observation, as well as the fact that the dataset has the least amount of images in this class\n",
        "- Class 3 defects seem to be scratches/indentations, typically vertical, going a shallow depth into the steel\n",
        "- Class 4 defects look like larger and deeper indentations/pits as compared to class 3\n",
        "\n",
        "*From trying potential pre-proccessing filters*\n",
        "\n",
        "Histogram equalization\n",
        "- Histogram equalization seems to work well for highlighting contrast around defects in classes 3 and 4 (which are typically larger than the defects in the other classes). The technique does not work well for class 2 or class 1 where the defects are smaller in size, and generally doesn't work well for any image where there is a lot of black space and the steel does not cover the whole image. \n",
        "- This suggests that images should be pre-processed to check for empty space and then have it removed. A CNN would be able to handle inputs of different sizes, but padding would need to be added around the output so all images have consistent sizing when going through any linear, fully-connected layers.\n",
        "\n",
        "Thresholding\n",
        "- This technique was generally unhelpful. Although it identified some contrast around the larger class 4 defects, it did not do it as well as histogram equalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Z6HLt8i6jt",
        "colab_type": "text"
      },
      "source": [
        "# **4. Split Filenames For Training, Validation and Testing Sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmo8EVNd_-uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split into training, validation, and testing set\n",
        "\n",
        "#First split up the filenames randomly. Use a 70-15-15 split within each class, append to the set, and then remove duplicates\n",
        "trainImageNames, valImageNames, testImageNames = [], [], []\n",
        "for i in range(0, numClasses):\n",
        "    filesForClass = filenamesByClass[i]\n",
        "    random.seed(1)\n",
        "    random.shuffle(filesForClass)\n",
        "\n",
        "    trainImageNames.append(filesForClass[:int(0.7*len(filesForClass))])\n",
        "    valImageNames.append(filesForClass[int(0.7*len(filesForClass)):int(0.85*len(filesForClass))])\n",
        "    testImageNames.append(filesForClass[int(0.85*len(filesForClass)):len(filesForClass)])\n",
        "\n",
        "#Flatten 2D lists to 1D\n",
        "trainImageNames = list(itertools.chain(*trainImageNames))\n",
        "valImageNames = list(itertools.chain(*valImageNames))\n",
        "testImageNames = list(itertools.chain(*testImageNames))\n",
        "\n",
        "#Remove duplicate namess\n",
        "trainImageNames = removeDuplicates(trainImageNames)\n",
        "valImageNames = removeDuplicates(valImageNames)\n",
        "testImageNames = removeDuplicates(testImageNames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwcPC8tMkIj2",
        "colab_type": "text"
      },
      "source": [
        "# **5. Load Images and Segmentation Masks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUWhbNSYiM1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Functions to convert encodedPixels string into a segmentation mask and vice versa\n",
        "def rle_decode(encodedPixels, picWidth, picHeight): \n",
        "    \n",
        "    numPixels = picWidth * picHeight\n",
        "    mask = np.zeros((picHeight, picWidth))\n",
        "    if encodedPixels == \"\":\n",
        "        return mask\n",
        "    \n",
        "    encodedPixelsList = encodedPixels.split()\n",
        "    for j in range(0, len(encodedPixelsList), 2): #iterate over locations within encodedPixels (every other value)\n",
        "        for k in range(1, int(encodedPixelsList[j+1]) + 1): #create range of the size that is the number of pixels to paint\n",
        "            pixelNum = int(encodedPixelsList[j]) + k - 1\n",
        "            xPos = math.floor(pixelNum / picHeight)\n",
        "            yPos = pixelNum % picHeight\n",
        "            mask[yPos, xPos] = 1\n",
        "    return mask\n",
        "\n",
        "def rle_encode(img):\n",
        "    '''\n",
        "    img: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels = img.flatten('F')\n",
        "    non_zero_indices = np.nonzero(pixels)[0]\n",
        "    rle = []\n",
        "    #Count numbers beside each other that are one more than the previous, remove them from non_zero_indices, \n",
        "    #then repeat until nothing is left of non_zero_indices and the full rle string is written\n",
        "    while np.size(non_zero_indices) > 1:\n",
        "        index = 0\n",
        "\n",
        "        while non_zero_indices[index + 1] - non_zero_indices[index] == 1:\n",
        "            index += 1\n",
        "            if index >= np.size(non_zero_indices) - 1:\n",
        "                break\n",
        "        index += 1\n",
        "        \n",
        "        rle.append(non_zero_indices[0])\n",
        "        rle.append(index)\n",
        "        non_zero_indices = non_zero_indices[index:]\n",
        "    return ' '.join(str(num) for num in rle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eV-42BwPBv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Custom dataset class to load the images, segmentation masks, and apply consistent transformations to them\n",
        "class SteelImageDataset(Dataset):\n",
        "    \"\"\"Steel image dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, filenames_list, rle_data, resize_width = int(1600/2), resize_height = int(256/2), apply_transforms = True, transform_apply_prob = 0.25, max_angle = 4):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            filenames_list (list): list of filenames for the class.\n",
        "            rle_data (list): original train.csv file loaded as a 2D list\n",
        "        \"\"\"\n",
        "        self.filenames_list = filenames_list\n",
        "        self.rle_data = rle_data\n",
        "        self.resize_width = resize_width\n",
        "        self.resize_height = resize_height\n",
        "        self.apply_transforms = apply_transforms #choose whether flips and rotations are done\n",
        "        self.transform_apply_prob = transform_apply_prob\n",
        "        self.max_angle = max_angle\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames_list)\n",
        "\n",
        "    def transform(self, image, masks):\n",
        "        new_masks, mask_tensors, temp = [], [], []\n",
        "        \n",
        "        # Resize\n",
        "        image = cv2.resize(image, (self.resize_width, self.resize_height))\n",
        "        for mask in masks:\n",
        "            temp.append(cv2.resize(mask, (self.resize_width, self.resize_height)))\n",
        "        new_masks = np.array(temp)\n",
        "        temp = []\n",
        "        \n",
        "        if self.apply_transforms:\n",
        "            # Random horizontal flipping with a probability of occuring 25% of the time\n",
        "            if np.random.random() < self.transform_apply_prob:\n",
        "                image = np.flip(image, 1)\n",
        "                for mask in new_masks:\n",
        "                    temp.append(np.flip(mask, 1))\n",
        "                new_masks = np.array(temp)\n",
        "                temp = []\n",
        "            \n",
        "            # Random vertical flipping with a probability of occuring 25% of the time\n",
        "            if np.random.random() < self.transform_apply_prob:\n",
        "                image = np.flip(image, 0)\n",
        "                for mask in new_masks:\n",
        "                    temp.append(np.flip(mask, 0))\n",
        "                new_masks = np.array(temp)\n",
        "                temp = []\n",
        "        \n",
        "            # Random rotations with a probability of occuring 25% of the time\n",
        "            if np.random.random() < self.transform_apply_prob:\n",
        "                angle = random.uniform(0, self.max_angle)\n",
        "                image = ndimage.rotate(image, angle, reshape = False)\n",
        "                for mask in new_masks:\n",
        "                    temp.append(ndimage.rotate(mask, angle, reshape = False))\n",
        "                new_masks = np.array(temp)\n",
        "                temp = []\n",
        "        \n",
        "        # Transform to tensor\n",
        "        image = torch.from_numpy(image.copy())\n",
        "        mask_tensors = torch.from_numpy(new_masks.copy())\n",
        "    \n",
        "        return image, mask_tensors\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        filename = self.filenames_list[idx]\n",
        "        img_name = \"/content/train_images/\" + filename\n",
        "        image = imageToRGBArray(getImage(img_name)) #Resnet needs a 3 channel img, so don't convert to RGB\n",
        "        #image = RGBToGrey(imageToRGBArray(getImage(img_name)))\n",
        "        masks, transformedMasks = [], []\n",
        "\n",
        "        for i in range(1, numClasses):       \n",
        "            masks.append(rle_decode(getEncodedPixels(self.filenames_list[idx], i), image.shape[1], image.shape[0]))\n",
        "        masks = np.asarray(masks)\n",
        "\n",
        "        image, transformedMasks = self.transform(image, masks)\n",
        "\n",
        "        return image, transformedMasks "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7FcMjHGSIFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = SteelImageDataset(trainImageNames, data)\n",
        "#val_dataset = SteelImageDataset(valImageNames, data)\n",
        "#test_dataset = SteelImageDataset(testImageNames, data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh5Atz5oTA7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a known image with defects in class 1 and 3\n",
        "image, masks = train_dataset[4137] \n",
        "plt.imshow(image.numpy(), cmap = 'gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs9pFFQnTPw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#verify that class 3 map is transformed correctly\n",
        "plt.imshow(masks.numpy()[2], cmap = 'gray') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMzlinE0f8AB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#verify that class 1 map is transformed correctly\n",
        "plt.imshow(masks.numpy()[0], cmap = 'gray') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmRVCQ3mmCUN",
        "colab_type": "text"
      },
      "source": [
        "# **6. Model Architectures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abNCFHPomKeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Baseline ANN model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBVPVvYImKnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPndJigV_-3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#U-Net Architecture (to use for final model) (taken from https://github.com/usuyama/pytorch-unet)\n",
        "\n",
        "def convrelu(in_channels, out_channels, kernel, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "class ResNetUNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "\n",
        "        self.base_model = models.resnet18(pretrained=True)\n",
        "        self.base_layers = list(self.base_model.children())\n",
        "\n",
        "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
        "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
        "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
        "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
        "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
        "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
        "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
        "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
        "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
        "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
        "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
        "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
        "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
        "\n",
        "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
        "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
        "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
        "\n",
        "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x_original = self.conv_original_size0(input)\n",
        "        x_original = self.conv_original_size1(x_original)\n",
        "\n",
        "        layer0 = self.layer0(input)\n",
        "        layer1 = self.layer1(layer0)\n",
        "        layer2 = self.layer2(layer1)\n",
        "        layer3 = self.layer3(layer2)\n",
        "        layer4 = self.layer4(layer3)\n",
        "\n",
        "        layer4 = self.layer4_1x1(layer4)\n",
        "        x = self.upsample(layer4)\n",
        "        layer3 = self.layer3_1x1(layer3)\n",
        "        x = torch.cat([x, layer3], dim=1)\n",
        "        x = self.conv_up3(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        layer2 = self.layer2_1x1(layer2)\n",
        "        x = torch.cat([x, layer2], dim=1)\n",
        "        x = self.conv_up2(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        layer1 = self.layer1_1x1(layer1)\n",
        "        x = torch.cat([x, layer1], dim=1)\n",
        "        x = self.conv_up1(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        layer0 = self.layer0_1x1(layer0)\n",
        "        x = torch.cat([x, layer0], dim=1)\n",
        "        x = self.conv_up0(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x_original], dim=1)\n",
        "        x = self.conv_original_size2(x)\n",
        "\n",
        "        out = self.conv_last(x)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuxUlWi-mM5r",
        "colab_type": "text"
      },
      "source": [
        "# **7. Training Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W17uZ83wlUKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#work in progress, not yet done\n",
        "def get_accuracy(model, data_loader, useGPU = True):\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, masks in data_loader:\n",
        "        #To Enable GPU Usage\n",
        "        if useGPU and torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "            model = model.cuda()\n",
        "   \n",
        "        output = model(imgs)\n",
        "        \n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "        #print(correct, \"out of\", total, \"correct predictions\")\n",
        "    return correct / total\n",
        "\n",
        "def plot_training_curve(iters, losses, train_acc, val_acc):\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_acc, label=\"Train\")\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))  \n",
        "\n",
        "def train(model, train_dataset, val_dataset, batch_size=64, num_epochs=1, learning_rate = 0.01, momentum = 0.9, useGPU = True, saveWeights = True, useAdams = False):\n",
        "    \n",
        "    #Put data in data loaders\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
        "                                            num_workers=0, shuffle=False)\n",
        "    val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n",
        "                                        num_workers=0, shuffle=False)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if useAdams:\n",
        "        optimizer = optim.Adam(model.parameters(), learning_rate)\n",
        "    else:\n",
        "        optimizer = optim.SGD(model.parameters(), learning_rate, momentum)\n",
        "    \n",
        "    \n",
        "    iters, losses, train_acc, val_acc = [], [], [], []\n",
        "\n",
        "    if useGPU and torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        print(\"Training on GPU\")\n",
        "   \n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, batch in enumerate(train_data_loader):\n",
        "            imgs, masks = batch\n",
        "            \n",
        "            imgs = imgs.permute(0, 3, 1, 2) #N, C, H, W format instead of N, H, W, C\n",
        "            imgs = imgs.float()\n",
        "            #imgs = imgs.unsqueeze(1) # pytorch needs the channel (see https://stackoverflow.com/questions/56789038/runtimeerror-given-groups-1-weight-of-size-64-3-3-3-expected-input4-50)\n",
        "            \n",
        "            #To Enable GPU Usage\n",
        "            if useGPU and torch.cuda.is_available():\n",
        "                imgs = imgs.cuda()\n",
        "                masks = masks.cuda()\n",
        "\n",
        "            out = model(imgs)             # forward pass\n",
        "\n",
        "            loss = criterion(out, masks) # compute the total loss\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
        "            train_acc.append(get_accuracy(model, train_data_loader, useGPU)) # compute training accuracy \n",
        "            val_acc.append(get_accuracy(model, val_data_loader, useGPU))  # compute validation accuracy\n",
        "            print(\"Iteration: \", str(n), \"| Train Loss: \", losses[n], \"| Train Accuracy: \", train_acc[n], \"| Validation Accuracy: \", val_acc[n])\n",
        "            \n",
        "            n += 1\n",
        "            \n",
        "        # Save the current model (checkpoint) to a file\n",
        "        if saveWeights:\n",
        "            model_path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name,\n",
        "                                                       batch_size,\n",
        "                                                       str(learning_rate).replace('.', '-'),\n",
        "                                                       epoch)\n",
        "            torch.save(model.state_dict(), model_path + \".pth\")\n",
        "            \n",
        "    # Write the train/test loss/err into CSV file for plotting later\n",
        "    if saveWeights:\n",
        "        epochs = np.arange(1, num_epochs + 1)\n",
        "        np.savetxt(\"{}_train_loss.csv\".format(model_path), losses)\n",
        "        np.savetxt(\"{}_train_acc.csv\".format(model_path), train_acc)\n",
        "        np.savetxt(\"{}_val_acc.csv\".format(model_path), val_acc)            \n",
        "\n",
        "    # plotting\n",
        "    plot_training_curve(iters, losses, train_acc, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT4Ww5BU4rY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ResNetUNet(4)\n",
        "train(model, train_dataset, train_dataset, batch_size = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HjoyPapilD9",
        "colab_type": "text"
      },
      "source": [
        "# X. Functions for painting defects "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmVTf-qcCl8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to load, paint, and save pictures given a list of filenames. Outputs jpg file in folder specified within function\n",
        "\n",
        "def loadPaintAndSavePictures(filenames, filenameAndClassIndex):\n",
        "    #Defect colours by class: 1 - Red, 2 - Green, 3 - Blue, 4 - Yellow. Opacity: 1 = transparent, 255 = opaque\n",
        "    opacity = 30\n",
        "    colour = [(255, 0, 0, opacity), (0, 255, 0, opacity), (0, 0, 255, opacity), (255, 255, 0, opacity)]\n",
        "\n",
        "    for file in filenames:\n",
        "        #Load the picture\n",
        "        picture = getImage(file)\n",
        "\n",
        "        # Paint the picture for each defect type\n",
        "        for i in range(1,5):\n",
        "            if file + \"_\" + str(i) in filenameAndClassIndex.keys():\n",
        "                encodedPixels = getEncodedPixels(file, i)\n",
        "                picture = applyMask(picture, encodedPixels, colour[i - 1])\n",
        "\n",
        "        #Save the picture\n",
        "        highlightsPath = \"/content/train_images_highlighted_defects\" \n",
        "        if not os.path.exists(highlightsPath):\n",
        "            os.makedirs(highlightsPath)\n",
        "        os.chdir(highlightsPath)\n",
        "        picture.save(file.split(\".jpg\")[0] + \" highlighted.jpg\")\n",
        "\n",
        "    print(\"Highlighting completed\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slj-7Qa6yInT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Paint the first x pictures from the dataset\n",
        "\n",
        "x  = 4\n",
        "loadPaintAndSavePictures(filenames[:x], filenameAndClassIndex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoaV46DZAcf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Zip the highlighted images to a folder\n",
        "!zip -r \"/content/train_images_highlighted_defects.zip\"  \"/content/train_images_highlighted_defects\" -q"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}